{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from splinter import Browser\n",
    "from bs4 import BeautifulSoup as soup\n",
    "import lxml\n",
    "from bs4 import NavigableString\n",
    "import re\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "import psycopg2\n",
    "\n",
    "from config import db_password\n",
    "\n",
    "# Must gather positive, neutral, and negative ratings numbers for both users and critics as well as the\n",
    "# metascore and userscore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!which chromedriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser = Browser(\"chrome\", executable_path=\"/usr/local/bin/chromedriver\", headless=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "games_df = pd.read_csv(\"../Resources/vgsales.csv.zip\")\n",
    "list_of_games = games_df[\"Name\"].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Navigating to the correct page via the search bar using splinter is too slow. I will rewrite the code to manually \n",
    "# write in the url using string interpolation. \n",
    "# I will need to interpolate the correct console name as well into the url. Metacritics url structure is:\n",
    "# \"https://www.metacritic.com/game/{console}/{game}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# must change game names into the proper format to be interpolated into the URL.\n",
    "# No SNES \"Super Nintendo Entertainment System\", SAT (Sega Saturn), NES, GB\n",
    "\n",
    "game_conversions= {\n",
    "    \"PSV\":\"playstation-vita\",\n",
    "    \"DC\":\"dreamcast\",\n",
    "    \"WiiU\":\"wii-u\",\n",
    "    \"GC\":\"gamecube\",\n",
    "    \"N64\":\"nintendo-64\",\n",
    "    \"XB\":\"xbox\",\n",
    "    \"PSP\":\"psp\",\n",
    "    \"PS4\":\"playstation-4\",\n",
    "    \"PS\": \"playstation\",\n",
    "    \"Wii\":\"wii\",\n",
    "    \"PS3\": \"playstation-3\",\n",
    "    \"PS2\":\"playstation-2\",\n",
    "    \"GBA\":\"game-boy-advance\",\n",
    "    \"DS\":\"ds\",\n",
    "    \"XB\":\"xbox\",\n",
    "    \"X360\":\"xbox-360\",\n",
    "    \"XOne\":\"xbox-one\",\n",
    "    'PC':\"pc\",\n",
    "    '3DS':\"3ds\",\n",
    "    }\n",
    "\n",
    "def convert_to_name(platform_abb):\n",
    "    if platform_abb in game_conversions:\n",
    "        full_platform= game_conversions[platform_abb]\n",
    "    else:\n",
    "        full_platform=\"other\"\n",
    "    return full_platform\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This actually shows that the vast majority of our data belongs to the consoles listed on metacritic, these \"legacy consoles\"\n",
    "# Get rid of those games which fall into the \"other\" category under full_platform_name\n",
    "games_df[\"full_platform_name\"]= games_df[\"Platform\"].map(convert_to_name)\n",
    "games_df = games_df.loc[games_df[\"full_platform_name\"]!=\"other\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "games_df[\"Name\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This worked pretty well, but I will alter the code further see if I can increase my scraping success to fail ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser = Browser(\"chrome\", executable_path=\"/usr/local/bin/chromedriver\", headless=True)\n",
    "problem_games_counter= 0\n",
    "problem_games = []\n",
    "game_info_list = []\n",
    "counter = 1\n",
    "abberant_side_details_counter = 0\n",
    "\n",
    "problem_metascore=[]\n",
    "problem_userscore=[]\n",
    "problem_release_date=[]\n",
    "problem_games_scores=[]\n",
    "problem_user_scores=[]\n",
    "abberant_side_detail_games = []\n",
    "\n",
    "\n",
    "# zip the two \n",
    "zipped_list =list(zip(games_df[\"full_platform_name\"].tolist(),games_df[\"Name\"].tolist()))\n",
    "\n",
    "for platform, name in zipped_list:\n",
    "    game_scores = {}\n",
    "    x = re.compile(\"\\.|:|;|\\s|&\")\n",
    "    edited_game_title= re.sub(x,\"-\", name).lower().replace(\"---\",\"-\").replace(\"--\",\"-\").replace(\"'\", \"\")\n",
    "    # periods at the end of the url will show up as a dash. This conditional is to get rid of it.\n",
    "    if edited_game_title[-1] == \"-\":\n",
    "        edited_game_title = edited_game_title[:-1]\n",
    "        print(f\"\\nGAME TITLE: {edited_game_title}\")\n",
    "    else:\n",
    "        print(f\"\\nGAME TITLE: {edited_game_title}\")\n",
    "    browser.visit(f\"https://www.metacritic.com/game/{platform}/{edited_game_title}\")\n",
    "    # fill in my search term to the Metacritic Browser and us \"\\n\" to press \"enter\" and initialize search\n",
    "    \n",
    "    game_scores[\"Name\"]=name\n",
    "    game_scores[\"Platform\"]=platform\n",
    "    \n",
    "    html= browser.html\n",
    "    my_soup = soup(html, \"lxml\")\n",
    "    # Grabbing the Metascore\n",
    "    try:\n",
    "        metascore = my_soup.find(\"a\", class_=\"metascore_anchor\").get_text()\n",
    "        game_scores[\"metascore\"]= metascore\n",
    "    except:\n",
    "        problem_metascore.append(name)\n",
    "        print('THIS IS A PROBLEM CHILD')\n",
    "        counter+=1\n",
    "        continue\n",
    "    # Grabbing the Userscore\n",
    "    try:\n",
    "        userscore_first = my_soup.find(\"div\", class_=\"details side_details\")\n",
    "        user_score = userscore_first.select(\"div.metascore_w\")[0].get_text()\n",
    "        game_scores[\"user_score\"]= user_score\n",
    "    except:\n",
    "        problem_userscore.append(name)\n",
    "        print(\"THIS IS A PROBLEM CHILD\")\n",
    "        counter+=1\n",
    "        continue\n",
    "    # Grabbing the release data\n",
    "    try:\n",
    "        product_data = my_soup.find(\"div\",class_=\"product_data\")\n",
    "        release_data = product_data.find(\"li\", class_=\"summary_detail release_data\")\n",
    "        release_date = release_data.find(\"span\",class_=\"data\").get_text()\n",
    "        game_scores[\"release_date\"]=release_date\n",
    "    except:\n",
    "        problem_release_date.append(name)\n",
    "        print(\"THIS IS A PROBLEM CHILD\")\n",
    "        counter+=1\n",
    "        continue\n",
    "    # Grabbing the positive, mixed, and negative critic review numbers\n",
    "    try:\n",
    "        critic_reviews = []\n",
    "        positive_scores = my_soup.find(\"div\",class_=\"distribution_wrap\")\n",
    "        x = positive_scores.select(\"ol.score_counts.hover_none span.count\")\n",
    "        for i in x:\n",
    "            critic_reviews.append(i.get_text())\n",
    "        game_scores[\"positive_critics\"] = critic_reviews[0]\n",
    "        game_scores[\"neutral_critics\"]=critic_reviews[1]\n",
    "        game_scores[\"negative_critics\"]=critic_reviews[2]\n",
    "    except:\n",
    "        problem_games_scores.append(name)\n",
    "        print('THIS IS A PROBLEM CHILD')\n",
    "        counter+=1\n",
    "        continue\n",
    "    # Grabbing the positive, mixed, and negative user review numbers\n",
    "    try:\n",
    "        user_reviews = []\n",
    "        user_scores = my_soup.find(\"div\",class_=\"user_reviews_module\")\n",
    "        drilling_down = user_scores.find(\"div\",class_=\"distribution_wrap\")\n",
    "        ordered_list_html = drilling_down.select(\"ol.score_counts.hover_none span.count\")\n",
    "        for i in ordered_list_html:\n",
    "            user_reviews.append(i.get_text())\n",
    "        game_scores[\"positive_users\"] = user_reviews[0]\n",
    "        game_scores[\"neutral_users\"]=user_reviews[1]\n",
    "        game_scores[\"negative_users\"]=user_reviews[2]\n",
    "    except:\n",
    "        problem_user_scores.append(name)\n",
    "        print(\"THIS IS A PROBLEM CHILD\")\n",
    "        counter+=1\n",
    "        continue\n",
    "    # Grabbing side details\n",
    "    try:\n",
    "        i=0\n",
    "        side_details = my_soup.find_all(\"div\", class_=\"details side_details\")[1]\n",
    "        unordered_list = side_details.select(\"ul.summary_details li\")\n",
    "        developer = unordered_list[0].find(\"span\", class_=\"data\").get_text()\n",
    "        game_scores[\"developer\"]=developer\n",
    "        try:\n",
    "            number_players = unordered_list[2].find(\"span\",class_=\"data\").get_text()\n",
    "            if number_players != \"On GameFAQs\":\n",
    "                game_scores[\"number_players\"]=number_players\n",
    "            else:\n",
    "                i = i-1\n",
    "        except:\n",
    "            i= i-1\n",
    "            print(\"mutliplayer not listed\")\n",
    "        rating = unordered_list[4+i].find(\"span\", class_=\"data\").get_text()\n",
    "        game_scores[\"rating\"]=rating\n",
    "        game_info_list.append(game_scores)\n",
    "        print('SUCCESSFUL')\n",
    "        print(f\"This is the {counter}th game in the list.\")\n",
    "        print(abberant_side_details_counter)\n",
    "        counter+=1\n",
    "    except:\n",
    "        abberant_side_detail_games.append(name)\n",
    "        print(\"SUCCESS but no side details\")\n",
    "        abberant_side_details_counter+=1\n",
    "        continue\n",
    "        \n",
    "            \n",
    "ratings_df = pd.DataFrame(game_info_list)\n",
    "ratings_df.to_csv(\"../Resources/final_scrape5.csv\")\n",
    "\n",
    "problem_ratings = pd.DataFrame(problem_games)\n",
    "problem_ratings.to_csv(\"../Resources/final_problems5.csv\")\n",
    "\n",
    "\n",
    "db_string = f\"postgres://postgres:{db_password}@final-project-db.celqxz4aecqm.us-east-1.rds.amazonaws.com/games_db\" \n",
    "engine = create_engine(db_string)\n",
    "ratings_df.to_sql(name=\"ratings\", con=engine, if_exists=\"replace\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PythonData",
   "language": "python",
   "name": "pythondata"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
